# backend/api/feedback.py

import json
from typing import List, Optional, Dict, Any

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

# STTì—ì„œ ì´ë¯¸ ë§Œë“  OpenAI client ì¬ì‚¬ìš©
from backend.api.stt import client as openai_client

# DB ê´€ë ¨
from backend.db import get_db
from backend.models.feedback_models import CoachEval, CoachMemo


# ---------- ìŠ¤ì¼€ì¼ ì„¤ì • (OSAD + OMP) ----------

SCALE_CONFIG: Dict[str, Dict[str, Any]] = {
    # 1) ê¸°ë³¸ OSAD (ì§€ë„ì „ë¬¸ì˜ í”¼ë“œë°±ìš©)
    "OSAD_DEBRIEFER": {
        "id": "OSAD_DEBRIEFER",
        "label": "OSAD (Objective Structured Assessment of Debriefing) for Debriefer",
        "max_per_item": 5,
        "num_items": 9,
        "max_total": 45,  # 9ê°œ í•­ëª© Ã— 5ì  = 45
        "dimensions": [
            "approach",
            "learning_env",
            "engagement",
            "reaction",
            "reflection",
            "analysis",
            "diagnosis",
            "application",
            "summary",
        ],
        # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— OSAD í•­ëª© ì„¤ëª…ë„ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
        # "dimension_labels": { ... }
    },

    # 2) OMP ì„ìƒ í”¼ë“œë°± ìŠ¤ì¼€ì¼ (ì›í˜• 5 microskills)
    "OMP_CLINICAL": {
        "id": "OMP_CLINICAL",
        "label": "One-Minute Preceptor (OMP) Clinical Teaching Scale",
        "max_per_item": 5,
        "num_items": 5,
        "max_total": 25,  # 5ê°œ í•­ëª© Ã— 5ì  = 25
        # JSON ì•ˆì—ì„œ ì‚¬ìš©í•  í‚¤ ì´ë¦„ë“¤
        "dimensions": [
            "get_commitment",
            "probe_for_evidence",
            "teach_general_rules",
            "reinforce_what_was_done_right",
            "correct_mistakes",
        ],
        # ğŸ”¹ ê° í•­ëª© ì œëª©: í•œê¸€ + (ì˜ì–´ ì›ë¬¸) ë³‘ê¸°
        "dimension_labels": {
            "get_commitment": "ì˜ê²¬Â·ì§„ë‹¨Â·ê³„íšì— ëŒ€í•œ ì „ê³µì˜ ì…ì¥ ëŒì–´ë‚´ê¸° (Get a commitment)",
            "probe_for_evidence": "íŒë‹¨ì˜ ê·¼ê±°ë¥¼ ì§ˆë¬¸í•˜ê³  íƒìƒ‰í•˜ê¸° (Probe for supporting evidence)",
            "teach_general_rules": "ì ìš© ê°€ëŠ¥í•œ ì¼ë°˜ ì›ì¹™/ê·œì¹™ì„ ê°€ë¥´ì¹˜ê¸° (Teach general rules)",
            "reinforce_what_was_done_right": "ì˜í•œ ë¶€ë¶„ì„ êµ¬ì²´ì ìœ¼ë¡œ ê°•í™”í•˜ê¸° (Reinforce what was done right)",
            "correct_mistakes": "ì‹¤ìˆ˜ë‚˜ ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë°”ë¡œì¡ì•„ ì£¼ê¸° (Correct mistakes)",
        },
    },
}


# ---------- Pydantic ëª¨ë¸ ----------

class Segment(BaseModel):
    speaker: str
    start: Optional[float] = None
    end: Optional[float] = None
    text: str


class FeedbackContext(BaseModel):
    case: Optional[str] = None
    language: Optional[str] = None
    note: Optional[str] = None


class FeedbackRequest(BaseModel):
    encounter_id: Optional[str] = None
    supervisor_id: Optional[str] = None
    trainee_id: Optional[str] = None
    audio_ref: Optional[str] = None

    # ìŠ¤ì¼€ì¼/ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ (ì¼ë°˜í™” í¬ì¸íŠ¸)
    scale_code: Optional[str] = "OSAD_DEBRIEFER"
    scenario_code: Optional[str] = "EM_DEBRIEF"

    transcript: str = Field(..., description="ì „ì²´ ëŒ€í™” transcript")
    trainee_level: Optional[str] = "PGY-2"
    language: str = "ko"

    context: Optional[FeedbackContext] = None
    segments: Optional[List[Segment]] = None
    # í”„ë¡ íŠ¸ì—ì„œ ë³´ë‚´ëŠ” SPEAKER_00 â†’ "ì§€ë„ì „ë¬¸ì˜"/"ì „ê³µì˜" ë§¤í•‘
    speaker_mapping: Optional[Dict[str, str]] = None


# ì½”ì¹­ ë¦¬í¬íŠ¸ì— ëŒ€í•œ ì „ë°˜ì  ë„ì›€ ì •ë„(1~5ì )ë¥¼ ë°›ëŠ” ìš”ì²­ ëª¨ë¸
class CoachEvalRequest(BaseModel):
    encounter_id: Optional[str] = None
    supervisor_id: Optional[str] = None
    trainee_id: Optional[str] = None

    scenario_code: str = "EM_DEBRIEF"
    scale_code: str = "OSAD_DEBRIEFER"
    model_version: Optional[str] = "gpt-4o-mini-osad-v1"

    helpful_score: int = Field(..., ge=1, le=5, description="1~5ì  Likert")
    # í”„ë¡ íŠ¸ì—ì„œ "ê¸°ë¡"ìœ¼ë¡œ ì²´í¬í•œ í•­ëª©ë“¤ì„ helpful_flagsë¡œ ê°™ì´ ë³´ë‚¼ ìˆ˜ ìˆìŒ
    helpful_flags: Optional[List[str]] = None
    comment: Optional[str] = None


# ì½”ì¹­ ë¦¬í¬íŠ¸ì˜ íŠ¹ì • ì„¹ì…˜(ê°•ì /ê°œì„ ì /ìŠ¤í¬ë¦½íŠ¸/ë¯¸ì„¸ìŠµê´€)ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš”ì²­ ëª¨ë¸
class CoachMemoRequest(BaseModel):
    encounter_id: Optional[str] = None
    supervisor_id: Optional[str] = None
    trainee_id: Optional[str] = None

    scenario_code: str = "EM_DEBRIEF"
    scale_code: str = "OSAD_DEBRIEFER"
    model_version: Optional[str] = "gpt-4o-mini-osad-v1"

    # saved_sections: {
    #   "strengths": "...\n...",
    #   "improvements_top3": "...\n...",
    #   "script_next_time": "...",
    #   "micro_habit_10sec": "..."
    # }
    saved_sections: Dict[str, str]
    note: Optional[str] = None


router = APIRouter(tags=["feedback"])


@router.post("/feedback")
async def analyze_feedback(payload: FeedbackRequest) -> Dict[str, Any]:
    """
    í”¼ë“œë°± ëŒ€í™”ë¥¼ (ê¸°ë³¸: OSAD_DEBRIEFER ìŠ¤ì¼€ì¼, ì„ íƒ ì‹œ: OMP_CLINICAL ë“±)
    ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ê° í•­ëª©ì˜ ê·¼ê±°ê°€ ëœ segment indexë¥¼ evidenceë¡œ í•¨ê»˜ ëŒë ¤ì¤€ë‹¤.
    """

    transcript = payload.transcript.strip()

    # ---------- ì–´ë–¤ ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í• ì§€ ê²°ì • ----------
    scale_code = (payload.scale_code or "OSAD_DEBRIEFER").upper()
    if scale_code not in SCALE_CONFIG:
        scale_code = "OSAD_DEBRIEFER"
    scale_cfg = SCALE_CONFIG[scale_code]
    max_total = scale_cfg["max_total"]
    dimensions: List[str] = scale_cfg["dimensions"]
    dimension_labels: Dict[str, str] = scale_cfg.get("dimension_labels", {})

    # ---------- JSON ìŠ¤í‚¤ë§ˆ(ì ìˆ˜ / evidence) ë¬¸ìì—´ ë™ì  ìƒì„± ----------
    # ì ìˆ˜ ë¶€ë¶„: "osad": { "<dim>": int(1-5), ... }
    score_schema_lines: List[str] = []
    for dim in dimensions:
        score_schema_lines.append(f'    "{dim}": int (1-5),\n')
    score_schema_text = "".join(score_schema_lines)

    # evidence ë¶€ë¶„: "evidence": { "osad": { "<dim>": [int, ...], ... } }
    ev_schema_lines: List[str] = []
    for dim in dimensions:
        ev_schema_lines.append(f'      "{dim}": [int, ...],\n')
    evidence_schema_text = "".join(ev_schema_lines)

    # í”„ë¡¬í”„íŠ¸ì— ë³´ì—¬ì¤„ ìŠ¤ì¼€ì¼ í•­ëª© ì„¤ëª… (ìˆìœ¼ë©´)
    dimension_desc_text = ""
    if dimension_labels:
        desc_lines = []
        for dim in dimensions:
            label = dimension_labels.get(dim, dim)
            desc_lines.append(f"- {dim}: {label}")
        dimension_desc_text = "\n".join(desc_lines)

    # ---------- segments ì „ì²´ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ë¬¸ìì—´ë¡œ ë‚˜ì—´ ----------
    if payload.segments:
        lines = []
        for idx, seg in enumerate(payload.segments):
            lines.append(
                f"[{idx}] speaker={seg.speaker}, "
                f"start={seg.start}, end={seg.end}, text=\"{seg.text}\""
            )
        segments_desc = "\n".join(lines)
    else:
        segments_desc = "(segments not provided)"

    # ---------- speaker_mappingì„ ì´ìš©í•´ 'ì§€ë„ì „ë¬¸ì˜ ë°œì–¸'ë§Œ ë”°ë¡œ ëª¨ìœ¼ê¸° ----------
    supervisor_only_text = ""
    if payload.segments and payload.speaker_mapping:
        supervisor_lines: List[str] = []
        for seg in payload.segments:
            role = payload.speaker_mapping.get(seg.speaker)
            if role == "ì§€ë„ì „ë¬¸ì˜":
                supervisor_lines.append(seg.text)
        if supervisor_lines:
            supervisor_only_text = "\n".join(supervisor_lines)

    context_desc = ""
    if payload.context:
        context_desc = (
            f"case={payload.context.case}, "
            f"note={payload.context.note}"
        )

    # ---------- ì¶œë ¥ ì–¸ì–´ ê²°ì • ----------
    lang_code = (payload.language or "ko").lower()
    lang_name_map = {
        "ko": "Korean",
        "en": "English",
        "zh": "Chinese",
        "es": "Spanish",
        "ja": "Japanese",
        "fr": "French",
        "de": "German",
        "auto": "auto",
    }
    output_lang_name = lang_name_map.get(
        lang_code, "the same language as the conversation"
    )

    # ---------- ì–¸ì–´ ì§€ì¹¨ ë¬¸ì¥ ----------
    if lang_code == "auto":
        lang_instruction = (
            "Infer the primary language used by the supervisor in the conversation "
            "(especially from the 'Supervisor-only speech' section). "
            "Write all explanation texts (strings) in that language. "
            "If you cannot clearly infer the language, default to Korean."
        )
    else:
        lang_instruction = (
            f"Write all explanation texts (strings) in {output_lang_name}."
        )

    # ---------- system í”„ë¡¬í”„íŠ¸ ----------
    system_prompt = (
        "You are an expert in medical education and feedback.\n"
        f"You are now using a feedback scale with code: {scale_code}, "
        f"label: {scale_cfg['label']}.\n"
        "You analyze a debriefing/feedback conversation between a supervisor "
        "and a trainee (resident), then score it and provide coaching tips.\n\n"
    )

    if dimension_desc_text:
        system_prompt += "This scale has the following dimensions:\n"
        system_prompt += dimension_desc_text + "\n\n"

    system_prompt += (
        "You MUST reply in a single valid JSON object ONLY, with this schema:\n"
        "{\n"
        '  "osad": {\n'
        f"{score_schema_text}"
        '    "total": int,\n'
        f'    "scale": int (use {max_total} as the maximum total score for this scale),\n'
        '    "percent": number (0-100, optional)\n'
        "  },\n"
        '  "structure": {\n'
        '    "has_opening": bool,\n'
        '    "has_core": bool,\n'
        '    "has_closing": bool\n'
        "  },\n"
        '  "coach": {\n'
        '    "strengths": [string, ...],\n'
        '    "improvements_top3": [string, ...],\n'
        '    "script_next_time": string,\n'
        '    "micro_habit_10sec": string\n'
        "  },\n"
        '  "evidence": {\n'
        '    "osad": {\n'
        f"{evidence_schema_text}"
        "    }\n"
        "  }\n"
        "}\n\n"
        "All evidence indices must refer to the segment indices given in the input.\n"
        "Use only indices that exist. If there is no clear evidence, use an empty list.\n"
        f"{lang_instruction}\n"
        "If only the supervisor's speech is provided separately, "
        "focus your scoring and coaching mainly on the supervisor's feedback behaviour.\n"
    )

    # ---------- user í”„ë¡¬í”„íŠ¸ ----------
    user_prompt_parts = [
        f"Language code from client: {payload.language}",
        f"Trainee level: {payload.trainee_level}",
        f"Scenario code: {payload.scenario_code}",
        f"Scale code: {scale_code}",
        f"Context: {context_desc}",
        "",
        "Full conversation transcript:",
        "------------------------------------",
        transcript,
        "",
        "Segments with indices:",
        "------------------------------------",
        segments_desc,
    ]

    if supervisor_only_text:
        user_prompt_parts.extend(
            [
                "",
                "Supervisor-only speech (extracted from segments based on speaker_mapping):",
                "------------------------------------",
                supervisor_only_text,
                "",
                "When scoring and generating coaching tips, "
                "prioritize the supervisor-only speech above.",
            ]
        )

    user_prompt_parts.append(
        "\nNow analyze this feedback conversation using the specified scale "
        "and respond ONLY with a JSON object following the required schema."
    )

    user_prompt = "\n".join(user_prompt_parts)

    try:
        # ---------- ChatCompletion í˜¸ì¶œ (JSON ëª¨ë“œ) ----------
        resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.2,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )

        content = resp.choices[0].message.content
        data = json.loads(content)

        # ---------- total / scale / percent ë³´ì • (ì—†ìœ¼ë©´ ê³„ì‚°) ----------
        osad = data.get("osad", {})

        # totalì´ ì—†ìœ¼ë©´ ìŠ¤ì¼€ì¼ì˜ dimensions ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ì‚°
        if "total" not in osad:
            numeric_scores: List[int] = []
            for dim in dimensions:
                val = osad.get(dim)
                if isinstance(val, int):
                    numeric_scores.append(val)
            osad["total"] = sum(numeric_scores)



        # í•­ìƒ ì´ ìŠ¤ì¼€ì¼ì˜ ë§Œì ì€ configì— ì •ì˜ëœ ê°’ìœ¼ë¡œ ê°•ì œ
        osad["scale"] = max_total

        total_val = osad.get("total")
        scale_val = max_total

        # percent(0~100%) ê³„ì‚°: (total / scale_val) * 100
        if isinstance(total_val, (int, float)) and scale_val > 0:
            osad.setdefault(
                "percent",
                round(total_val / scale_val * 100, 1),
            )
        else:
            osad.setdefault("percent", 0.0)

        data["osad"] = osad

        # ---------- evidence.osad ê¸°ë³¸ êµ¬ì¡° ë³´ì • ----------
        if "evidence" not in data:
            data["evidence"] = {"osad": {}}
        else:
            if "osad" not in data["evidence"]:
                data["evidence"]["osad"] = {}

        return data

    except json.JSONDecodeError as je:
        print("=== DEBUG: /feedback JSON decode error ===", repr(je))
        print("=== DEBUG: raw content ===", content)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse LLM JSON: {je}",
        )
    except Exception as e:
        print("=== DEBUG: /feedback ERROR ===", repr(e))
        raise HTTPException(
            status_code=500,
            detail=f"Feedback analysis failed: {e}",
        )


# ---------- ì½”ì¹­ ë¦¬í¬íŠ¸ì— ëŒ€í•œ ì „ë°˜ì  ë„ì›€ ì •ë„ í‰ê°€ ì €ì¥ ----------

@router.post("/feedback/coach-eval")
async def eval_coaching_report(
    payload: CoachEvalRequest,
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    """
    í”„ë¡ íŠ¸ì—ì„œ ë°›ì€ 'ì´ ì½”ì¹­ ë¦¬í¬íŠ¸ê°€ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ë˜ì—ˆëŠ”ì§€(1~5ì )' í‰ê°€ë¥¼
    coach_eval í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸.
    """
    try:
        # helpful_flagsëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        flags_json = None
        if payload.helpful_flags is not None:
            flags_json = json.dumps(payload.helpful_flags, ensure_ascii=False)

        obj = CoachEval(
            encounter_id=payload.encounter_id,
            supervisor_id=payload.supervisor_id,
            trainee_id=payload.trainee_id,
            scenario_code=payload.scenario_code,
            scale_code=payload.scale_code,
            model_version=payload.model_version,
            helpful_score=payload.helpful_score,
            helpful_flags=flags_json,
            comment=payload.comment,
        )
        db.add(obj)
        db.commit()
        db.refresh(obj)

        return {
            "status": "ok",
            "message": "coach-eval ì €ì¥ ì™„ë£Œ",
            "data": {
                "id": obj.id,
                "encounter_id": obj.encounter_id,
                "helpful_score": obj.helpful_score,
            },
        }
    except Exception as e:
        print("=== DEBUG: /feedback/coach-eval ERROR ===", repr(e))
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Coach evaluation save failed: {e}",
        )


# ---------- ì½”ì¹­ ë¦¬í¬íŠ¸ì—ì„œ 'ê¸°ë¡'ìœ¼ë¡œ ì²´í¬í•œ ì„¹ì…˜ ì €ì¥ ----------

@router.post("/feedback/coach-memo")
async def save_coaching_memo(
    payload: CoachMemoRequest,
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìê°€ 'ê¸°ë¡' ì²´í¬ë°•ìŠ¤ë¡œ ì„ íƒí•œ ì½”ì¹­ ë¦¬í¬íŠ¸ ì„¹ì…˜ì„
    coach_memo í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸.
    """
    try:
        # saved_sections(dict)ë¥¼ JSON ë¬¸ìì—´ë¡œ ì €ì¥
        sections_json = json.dumps(payload.saved_sections, ensure_ascii=False)

        obj = CoachMemo(
            encounter_id=payload.encounter_id,
            supervisor_id=payload.supervisor_id,
            trainee_id=payload.trainee_id,
            scenario_code=payload.scenario_code,
            scale_code=payload.scale_code,
            model_version=payload.model_version,
            saved_sections=sections_json,
            note=payload.note,
        )
        db.add(obj)
        db.commit()
        db.refresh(obj)

        return {
            "status": "ok",
            "message": "coach-memo ì €ì¥ ì™„ë£Œ",
            "data": {
                "id": obj.id,
                "encounter_id": obj.encounter_id,
            },
        }
    except Exception as e:
        print("=== DEBUG: /feedback/coach-memo ERROR ===", repr(e))
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Coach memo save failed: {e}",
        )
